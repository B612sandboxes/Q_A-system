import os
import time
import sys
from typing import List, Dict, Any
import traceback

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè§£å†³ç½‘ç»œé—®é¢˜[7,9]
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
# ç¦ç”¨è­¦å‘Šä¿¡æ¯ï¼Œå‡å°‘è¾“å‡ºå¹²æ‰°
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# å¯¼å…¥ä¿®å¤æ¨¡å—
try:
    from pwd_fix import fix_pwd_module
    fix_pwd_module()
except ImportError:
    print("âš ï¸  pwd_fixæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå¯èƒ½å½±å“Windowsç¯å¢ƒ")

# LangChainæ ¸å¿ƒç»„ä»¶ - ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥è·¯å¾„é¿å…å¼ƒç”¨è­¦å‘Š
try:
    from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_community.llms import Ollama
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    print("âœ… LangChainç»„ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ LangChainç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install langchain-community sentence-transformers")
    sys.exit(1)


class KnowledgeBaseQA:
    """çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, knowledge_base_path: str = "./knowledge_base"):
        self.knowledge_base = knowledge_base_path
        self.vector_db_path = "./vector_db"
        self.embeddings = None
        self.vector_store = None
        self.qa_chain = None
        self.llm = None
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(self.knowledge_base, exist_ok=True)
        os.makedirs(self.vector_db_path, exist_ok=True)
    
    def init_embeddings(self):
        """åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹ - ä¿®å¤ç½‘ç»œé—®é¢˜å’Œæ¨¡å‹è·¯å¾„[7,8](@ref)"""
        try:
            # æ­¥éª¤1ï¼šä½¿ç”¨å®Œæ•´çš„æ¨¡å‹è·¯å¾„ï¼Œé¿å…404é”™è¯¯
            model_name = "sentence-transformers/all-MiniLM-L6-v2"
            print(f"ğŸ”„ å°è¯•åŠ è½½æ¨¡å‹: {model_name}")
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={
                    'device': 'cpu',
                    'trust_remote_code': True  # æ·»åŠ ä¿¡ä»»è¿œç¨‹ä»£ç å‚æ•°
                },
                encode_kwargs={
                    'normalize_embeddings': True,
                    'show_progress_bar': False  # ç¦ç”¨è¿›åº¦æ¡å‡å°‘è¾“å‡º
                }
            )
            
            # æµ‹è¯•åµŒå…¥æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
            test_embedding = self.embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
            if len(test_embedding) > 0:
                print(f"âœ… æ–‡æœ¬åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
                return True
            else:
                raise Exception("åµŒå…¥æµ‹è¯•è¿”å›ç©ºç»“æœ")
                
        except Exception as e:
            print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸  ä½¿ç”¨ç¦»çº¿åµŒå…¥æ¨¡å‹...")
            return self.init_embeddings_offline()
    
    def init_embeddings_offline(self):
        """ç¦»çº¿æ¨¡å¼ä¸‹çš„åµŒå…¥æ¨¡å‹åˆå§‹åŒ– - ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥è·¯å¾„[7](@ref)"""
        try:
            # æ­¥éª¤2ï¼šä½¿ç”¨æ­£ç¡®çš„ç¤¾åŒºç‰ˆå¯¼å…¥
            from langchain_community.embeddings import FakeEmbeddings
            self.embeddings = FakeEmbeddings(size=384)
            print("âœ… ä½¿ç”¨ç¦»çº¿åµŒå…¥æ¨¡å‹ï¼ˆæ£€ç´¢è´¨é‡ä¼šé™ä½ï¼Œä½†ç³»ç»Ÿå¯è¿è¡Œï¼‰")
            return True
        except Exception as e:
            print(f"âŒ ç¦»çº¿åµŒå…¥æ¨¡å‹ä¹Ÿå¤±è´¥: {e}")
            # ç»ˆæå¤‡ç”¨æ–¹æ¡ˆï¼šåˆ›å»ºæœ€ç®€å•çš„åµŒå…¥ç±»
            class SimpleEmbeddings:
                def __init__(self, size=384):
                    self.size = size
                
                def embed_query(self, text):
                    # è¿”å›éšæœºå‘é‡ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                    import random
                    return [random.gauss(0, 1) for _ in range(self.size)]
                
                def embed_documents(self, texts):
                    return [self.embed_query(text) for text in texts]
            
            self.embeddings = SimpleEmbeddings()
            print("âœ… ä½¿ç”¨ç®€å•éšæœºåµŒå…¥æ¨¡å‹ï¼ˆä»…ä¿è¯ç³»ç»Ÿè¿è¡Œï¼‰")
            return True
    
    def load_documents(self) -> List:
        """åŠ è½½çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        if not os.path.exists(self.knowledge_base):
            print(f"âš ï¸  çŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.knowledge_base}")
            print(f"âœ… å·²åˆ›å»ºï¼Œè¯·å°†æ–‡æ¡£æ”¾å…¥æ­¤æ–‡ä»¶å¤¹")
            return []
        
        documents = []
        supported_extensions = {'.pdf', '.txt', '.docx', '.md'}
        
        # æ£€æŸ¥æ–‡ä»¶æ•°é‡
        files = os.listdir(self.knowledge_base)
        if not files:
            print("â„¹ï¸  çŸ¥è¯†åº“æ–‡ä»¶å¤¹ä¸ºç©º")
            return []
        
        print(f"ğŸ“ å‘ç°{len(files)}ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½æ”¯æŒçš„æ–‡æ¡£...")
        
        for filename in files:
            file_path = os.path.join(self.knowledge_base, filename)
            ext = os.path.splitext(filename)[1].lower()
            
            if ext not in supported_extensions:
                print(f"â­ï¸  è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filename}")
                continue
            
            try:
                if ext == '.pdf':
                    loader = PyPDFLoader(file_path)
                elif ext == '.txt':
                    # å°è¯•å¤šç§ç¼–ç [5](@ref)
                    loader = None
                    for encoding in ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']:
                        try:
                            loader = TextLoader(file_path, encoding=encoding)
                            break
                        except UnicodeDecodeError:
                            continue
                    if loader is None:
                        raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶: {filename}")
                elif ext == '.docx':
                    loader = Docx2txtLoader(file_path)
                else:
                    continue
                
                loaded_docs = loader.load()
                for doc in loaded_docs:
                    doc.metadata['source'] = filename
                    # ç¡®ä¿é¡µé¢ä¿¡æ¯å­˜åœ¨
                    if 'page' not in doc.metadata:
                        doc.metadata['page'] = 1
                
                documents.extend(loaded_docs)
                print(f"ğŸ“„ å·²åŠ è½½: {filename} ({len(loaded_docs)}ä¸ªç‰‡æ®µ)")
                
            except Exception as e:
                print(f"âŒ åŠ è½½{filename}å¤±è´¥: {e}")
                continue
        
        if documents:
            print(f"âœ… å…±åŠ è½½{len(documents)}ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        else:
            print("âš ï¸  æœªåŠ è½½ä»»ä½•æ–‡æ¡£ç‰‡æ®µ")
        return documents
    
    def split_documents(self, documents: List, chunk_size: int = 800, chunk_overlap: int = 100):
        """åˆ†å‰²æ–‡æ¡£ä¸ºå°å—"""
        if not documents:
            return []
            
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
        )
        split_docs = text_splitter.split_documents(documents)
        print(f"ğŸ“Š æ–‡æ¡£åˆ†å‰²å®Œæˆ: {len(documents)} -> {len(split_docs)} ä¸ªç‰‡æ®µ")
        return split_docs
    
    def create_vector_store(self, documents: List, force_recreate: bool = False):
        """åˆ›å»ºå‘é‡æ•°æ®åº“ """
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        faiss_index = os.path.join(self.vector_db_path, "index.faiss")
        pkl_index = os.path.join(self.vector_db_path, "index.pkl")
        
        if not force_recreate and os.path.exists(faiss_index) and os.path.exists(pkl_index):
            print("ğŸ“‚ åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
            try:
                # æ­¥éª¤3ï¼šä¿®å¤FAISSç‰ˆæœ¬å…¼å®¹æ€§ - å°è¯•ä¸åŒåŠ è½½æ–¹å¼
                self.vector_store = self._load_faiss_compatible()
                if self.vector_store:
                    print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
                    return True
                else:
                    print("ğŸ”„ å…¼å®¹æ€§åŠ è½½å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“...")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“...")
        
        if not documents:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯å¤„ç†")
            # å³ä½¿æ²¡æœ‰æ–‡æ¡£ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ªç©ºçš„å‘é‡å­˜å‚¨ï¼Œé¿å…åç»­é”™è¯¯
            try:
                from langchain_community.vectorstores import FAISS
                from langchain_community.embeddings import FakeEmbeddings
                temp_embeddings = FakeEmbeddings(size=384)
                # åˆ›å»ºç©ºçš„å‘é‡å­˜å‚¨
                self.vector_store = FAISS.from_texts(["ç³»ç»Ÿåˆå§‹åŒ–"], temp_embeddings)
                print("âœ… åˆ›å»ºç©ºå‘é‡æ•°æ®åº“å®Œæˆ")
                return True
            except Exception as e:
                print(f"âŒ åˆ›å»ºç©ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
                return False
        
        print("ğŸ”¨ åˆ›å»ºå‘é‡æ•°æ®åº“...")
        start_time = time.time()
        
        # åˆ†å‰²æ–‡æ¡£
        split_docs = self.split_documents(documents)
        if not split_docs:
            print("âŒ æ–‡æ¡£åˆ†å‰²åæ— å†…å®¹")
            return False
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        try:
            self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
            # ä¿å­˜åˆ°æœ¬åœ°
            self.vector_store.save_local(self.vector_db_path)
            creation_time = time.time() - start_time
            print(f"âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {creation_time:.2f}ç§’")
            return True
        except Exception as e:
            print(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def _load_faiss_compatible(self):
        """å…¼å®¹æ€§åŠ è½½FAISSå‘é‡æ•°æ®åº“ - å¤„ç†ä¸åŒç‰ˆæœ¬API[6](@ref)"""
        try:
            # æ–¹æ³•1ï¼šå°è¯•æ–°ç‰ˆæœ¬APIï¼ˆä¸å¸¦å±é™©ååºåˆ—åŒ–å‚æ•°ï¼‰
            try:
                vector_store = FAISS.load_local(self.vector_db_path, self.embeddings)
                print("âœ… ä½¿ç”¨æ–°ç‰ˆæœ¬APIåŠ è½½æˆåŠŸ")
                return vector_store
            except TypeError as e:
                if "allow_dangerous_deserialization" in str(e):
                    # æ–¹æ³•2ï¼šå°è¯•æ—§ç‰ˆæœ¬APIï¼ˆå¸¦å±é™©ååºåˆ—åŒ–å‚æ•°ï¼‰
                    try:
                        vector_store = FAISS.load_local(
                            self.vector_db_path, 
                            self.embeddings,
                            allow_dangerous_deserialization=True
                        )
                        print("âœ… ä½¿ç”¨æ—§ç‰ˆæœ¬APIåŠ è½½æˆåŠŸ")
                        return vector_store
                    except Exception as e2:
                        print(f"âŒ æ—§ç‰ˆæœ¬APIä¹Ÿå¤±è´¥: {e2}")
                        return None
                else:
                    # å…¶ä»–TypeErrorï¼Œé‡æ–°æŠ›å‡º
                    raise e
        except Exception as e:
            print(f"âŒ åŠ è½½å¼‚å¸¸: {e}")
            return None
    
    def init_llm(self, model_type: str = "ollama"):
        """åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ - å¢å¼ºé”™è¯¯å¤„ç†[8](@ref)"""
        try:
            if model_type == "ollama":
                # å°è¯•å¤šä¸ªå¯èƒ½çš„æ¨¡å‹åç§°
                model_names = ["qwen2.5:0.5b", "llama2", "qwen:7b", "mistral"]
                for model_name in model_names:
                    try:
                        self.llm = Ollama(model=model_name, temperature=0.1)
                        # æµ‹è¯•è¿æ¥ - ä½¿ç”¨æ›´ç®€å•çš„æµ‹è¯•æ–¹æ³•
                        test_response = self.llm.invoke("hello")
                        if test_response and len(test_response) > 0:
                            print(f"âœ… Ollamaæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
                            return
                        else:
                            raise Exception("æµ‹è¯•å“åº”ä¸ºç©º")
                    except Exception as e:
                        print(f"âš ï¸  æ¨¡å‹ {model_name} ä¸å¯ç”¨: {str(e)[:100]}...")
                        continue
                
                # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œå°è¯•æœ€åŸºæœ¬çš„è¿æ¥
                try:
                    self.llm = Ollama(model="qwen2.5:0.5b")
                    print("âš ï¸  ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆæœªæµ‹è¯•è¿æ¥ï¼‰")
                except:
                    # ç»ˆæå¤‡ç”¨æ–¹æ¡ˆ
                    from langchain_community.llms import FakeListLLM
                    self.llm = FakeListLLM(responses=["OllamaæœåŠ¡æœªæ­£ç¡®é…ç½®ï¼Œè¯·æ£€æŸ¥å®‰è£…å’Œè¿è¡ŒçŠ¶æ€"])
                    print("âš ï¸  ä½¿ç”¨æ¨¡æ‹ŸLLM")
                
            else:
                # å¤‡ç”¨ç®€å•æ¨¡å‹
                from langchain_community.llms import FakeListLLM
                self.llm = FakeListLLM(responses=["æŠ±æ­‰ï¼Œæ¨¡å‹æœªæ­£ç¡®é…ç½®"])
                print("âš ï¸  ä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
            from langchain_community.llms import FakeListLLM
            self.llm = FakeListLLM(responses=["æ¨¡å‹æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œç³»ç»Ÿå°†ä»¥åŸºç¡€æ¨¡å¼è¿è¡Œ"])
            print("âš ï¸  ä½¿ç”¨åŸºç¡€æ¨¡æ‹ŸLLM")
    
    def create_qa_chain(self):
        """åˆ›å»ºé—®ç­”é“¾ - å¢å¼ºé”™è¯¯å¤„ç†"""
        if not self.vector_store:
            print("âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return False
            
        if not self.llm:
            print("âŒ è¯­è¨€æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        # è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
        prompt_template = """åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·ç»™å‡ºå‡†ç¡®ã€ç®€æ´çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æ ¹æ®ä»¥ä¸Šä¸Šä¸‹æ–‡ç”¨ä¸­æ–‡å›ç­”ï¼š"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        try:
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_store.as_retriever(
                    search_kwargs={"k": 3}
                ),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True,
                verbose=False  # å‡å°‘è¾“å‡ºå™ªéŸ³
            )
            print("âœ… é—®ç­”é“¾åˆ›å»ºæˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ åˆ›å»ºé—®ç­”é“¾å¤±è´¥: {e}")
            return False
    
    def ask_question(self, question: str) -> Dict[str, Any]:
        """æé—®å¹¶è·å–ç­”æ¡ˆ - å¢å¼ºé”™è¯¯å¤„ç†"""
        if not self.qa_chain:
            return {
                "answer": "ç³»ç»Ÿæœªåˆå§‹åŒ–å®Œæˆï¼Œè¯·å…ˆè¿è¡Œåˆå§‹åŒ–æµç¨‹",
                "sources": [],
                "time": "0.00ç§’"
            }
        
        try:
            start_time = time.time()
            result = self.qa_chain({"query": question})
            response_time = time.time() - start_time
            
            answer = result.get("result", "æœªè·å¾—æœ‰æ•ˆç­”æ¡ˆ")
            sources = []
            
            # æå–æ¥æºæ–‡æ¡£
            source_docs = result.get("source_documents", [])
            for doc in source_docs:
                source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                page = doc.metadata.get("page", 1)
                content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                sources.append({
                    "source": source,
                    "page": page,
                    "preview": content_preview
                })
            
            return {
                "answer": answer,
                "sources": sources,
                "time": f"{response_time:.2f}ç§’"
            }
        except Exception as e:
            print(f"âŒ å›ç­”é—®é¢˜å‡ºé”™: {e}")
            return {
                "answer": f"ç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚: {str(e)[:100]}...",
                "sources": [],
                "time": "0.00ç§’"
            }
    
    def initialize_system(self, force_recreate: bool = False):
        """åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ - å¢å¼ºå¥å£®æ€§"""
        print("=" * 50)
        print("ğŸ¤– ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–ä¸­...")
        print("=" * 50)
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        print("æ­¥éª¤ 1/5: åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹...")
        if not self.init_embeddings():
            print("âš ï¸  åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œä½†å°è¯•ç»§ç»­è¿è¡Œ")
            # ä¸é€€å‡ºï¼Œå°è¯•ä½¿ç”¨åŸºç¡€åŠŸèƒ½
        
        # 2. åŠ è½½æ–‡æ¡£
        print("æ­¥éª¤ 2/5: åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£...")
        documents = self.load_documents()
        if not documents:
            print("âš ï¸  çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£æˆ–åŠ è½½å¤±è´¥")
            print("   æ”¯æŒæ ¼å¼: PDF, TXT, DOCX, MD")
            print("   è¯·å°†æ–‡æ¡£æ”¾å…¥ knowledge_base æ–‡ä»¶å¤¹")
            # ä¸é€€å‡ºï¼Œå…è®¸ä½¿ç”¨ç©ºçŸ¥è¯†åº“
        
        # 3. åˆ›å»ºå‘é‡æ•°æ®åº“
        print("æ­¥éª¤ 3/5: åˆ›å»ºå‘é‡æ•°æ®åº“...")
        if not self.create_vector_store(documents, force_recreate):
            print("âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥")
            # ä¸ç«‹å³é€€å‡ºï¼Œå°è¯•ç»§ç»­
        
        # 4. åˆå§‹åŒ–LLM
        print("æ­¥éª¤ 4/5: åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
        self.init_llm("ollama")
        
        # 5. åˆ›å»ºé—®ç­”é“¾
        print("æ­¥éª¤ 5/5: åˆ›å»ºé—®ç­”é“¾...")
        if not self.create_qa_chain():
            print("âŒ é—®ç­”é“¾åˆ›å»ºå¤±è´¥")
            return False
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        return True


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("ğŸ¯ ä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ v3.0 (ç»ˆæä¿®å¤ç‰ˆ)")
    print("=" * 50)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    qa_system = KnowledgeBaseQA()
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    print("ğŸš€ å¼€å§‹ç³»ç»Ÿåˆå§‹åŒ–...")
    success = qa_system.initialize_system()
    
    if success:
        print("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
    else:
        print("âš ï¸  ç³»ç»Ÿåˆå§‹åŒ–é‡åˆ°é—®é¢˜ï¼Œä½†å°è¯•ç»§ç»­è¿è¡ŒåŸºç¡€åŠŸèƒ½")
        print("ğŸ’¡ æ‚¨å¯ä»¥å°è¯•:")
        print("   1. æ£€æŸ¥çŸ¥è¯†åº“æ–‡æ¡£æ˜¯å¦å·²æ”¾å…¥ knowledge_base æ–‡ä»¶å¤¹")
        print("   2. ç¡®è®¤OllamaæœåŠ¡å·²å¯åŠ¨: ollama serve")
        print("   3. è¿è¡Œ 'é‡æ–°åŠ è½½' å‘½ä»¤é‡å»ºç³»ç»Ÿ")
    
    print("\nğŸ’¡ äº¤äº’æç¤º:")
    print("  ç›´æ¥è¾“å…¥é—®é¢˜: è·å–åŸºäºçŸ¥è¯†åº“çš„ç­”æ¡ˆ")
    print("  è¾“å…¥'å¸®åŠ©'æˆ–'help': æŸ¥çœ‹ä½¿ç”¨è¯´æ˜") 
    print("  è¾“å…¥'é‡æ–°åŠ è½½'æˆ–'reload': é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“")
    print("  è¾“å…¥'é€€å‡º'/'quit'/'exit': ç»“æŸç¨‹åº")
    print("-" * 50)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            
            if question.lower() in ['é€€å‡º', 'quit', 'exit']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            elif question.lower() in ['å¸®åŠ©', 'help']:
                print("\nğŸ“– ä½¿ç”¨è¯´æ˜:")
                print("  1. ç›´æ¥è¾“å…¥é—®é¢˜è·å–åŸºäºçŸ¥è¯†åº“çš„ç­”æ¡ˆ")
                print("  2. æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: PDF, TXT, DOCX, MD")
                print("  3. å°†æ–‡æ¡£æ”¾å…¥ knowledge_base æ–‡ä»¶å¤¹")
                print("  4. è¾“å…¥'é‡æ–°åŠ è½½'æ›´æ–°çŸ¥è¯†åº“ç´¢å¼•")
                print("  5. ç³»ç»Ÿä¼šæ˜¾ç¤ºç­”æ¡ˆæ¥æºå’Œå“åº”æ—¶é—´")
                continue
            
            elif question.lower() in ['é‡æ–°åŠ è½½', 'reload']:
                confirm = input("âš ï¸  ç¡®å®šè¦é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“å—ï¼Ÿ(y/N): ")
                if confirm.lower() in ['y', 'yes']:
                    print("ğŸ”„ é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿ...")
                    if qa_system.initialize_system(force_recreate=True):
                        print("âœ… ç³»ç»Ÿé‡æ–°åˆå§‹åŒ–å®Œæˆ")
                    else:
                        print("âŒ é‡æ–°åˆå§‹åŒ–å¤±è´¥")
                continue
            
            elif not question:
                continue
            
            # æé—®å¹¶è·å–ç­”æ¡ˆ
            print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ç­”æ¡ˆ...")
            result = qa_system.ask_question(question)
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            print(f"\nğŸ“ ç­”æ¡ˆ (å“åº”æ—¶é—´: {result['time']}):")
            print("-" * 50)
            print(result['answer'])
            print("-" * 50)
            
            # æ˜¾ç¤ºæ¥æº
            if result['sources']:
                print(f"\nğŸ“š å‚è€ƒæ¥æº:")
                for i, source in enumerate(result['sources'], 1):
                    print(f"  {i}. æ–‡æ¡£: {source['source']} (ç¬¬{source['page']}é¡µ)")
                    print(f"      æ‘˜è¦: {source['preview']}")
            else:
                print("\nâ„¹ï¸  æœªæ‰¾åˆ°ç›¸å…³æ¥æºæ–‡æ¡£")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            print("ğŸ’¡ ç³»ç»Ÿå°†ç»§ç»­è¿è¡Œï¼Œæ‚¨å¯ä»¥ç»§ç»­æé—®")
            traceback.print_exc()


if __name__ == "__main__":
    main()